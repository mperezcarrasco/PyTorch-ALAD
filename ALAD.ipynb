{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import ALADTrainer\n",
    "from preprocess import get_svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=100\n",
    "    lr=0.0002\n",
    "    latent_dim=100\n",
    "    normal_class=1\n",
    "    batch_size=32\n",
    "    pretrained=False\n",
    "    spec_norm=False\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_svhn(args)\n",
    "\n",
    "alad = ALADTrainer(args, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11088/11088: [===============================>] - ETA 0.4sss\n",
      "Training... Epoch: 0, Discrimiantor Loss: 1.982, Generator Loss: 21.107\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 1, Discrimiantor Loss: 1.156, Generator Loss: 29.986\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 2, Discrimiantor Loss: 0.573, Generator Loss: 37.794\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 3, Discrimiantor Loss: 0.636, Generator Loss: 36.059\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 4, Discrimiantor Loss: 0.477, Generator Loss: 34.553\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 5, Discrimiantor Loss: 0.798, Generator Loss: 32.608\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 6, Discrimiantor Loss: 0.846, Generator Loss: 32.106\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 7, Discrimiantor Loss: 0.666, Generator Loss: 33.281\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 8, Discrimiantor Loss: 0.689, Generator Loss: 35.366\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 9, Discrimiantor Loss: 0.673, Generator Loss: 34.399\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 10, Discrimiantor Loss: 0.855, Generator Loss: 33.598\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 11, Discrimiantor Loss: 0.611, Generator Loss: 33.980\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 12, Discrimiantor Loss: 0.677, Generator Loss: 35.187\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 13, Discrimiantor Loss: 0.492, Generator Loss: 37.104\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 14, Discrimiantor Loss: 0.584, Generator Loss: 37.921\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 15, Discrimiantor Loss: 0.570, Generator Loss: 36.051\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 16, Discrimiantor Loss: 0.528, Generator Loss: 37.305\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 17, Discrimiantor Loss: 0.466, Generator Loss: 37.299\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 18, Discrimiantor Loss: 0.429, Generator Loss: 40.934\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 19, Discrimiantor Loss: 0.603, Generator Loss: 40.195\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 20, Discrimiantor Loss: 0.496, Generator Loss: 35.622\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 21, Discrimiantor Loss: 0.406, Generator Loss: 41.081\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 22, Discrimiantor Loss: 0.564, Generator Loss: 36.962\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 23, Discrimiantor Loss: 0.411, Generator Loss: 39.396\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 24, Discrimiantor Loss: 0.457, Generator Loss: 39.430\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 25, Discrimiantor Loss: 0.387, Generator Loss: 42.780\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 26, Discrimiantor Loss: 0.415, Generator Loss: 45.315\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 27, Discrimiantor Loss: 0.556, Generator Loss: 38.463\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 28, Discrimiantor Loss: 0.502, Generator Loss: 38.171\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 29, Discrimiantor Loss: 0.512, Generator Loss: 38.866\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 30, Discrimiantor Loss: 0.522, Generator Loss: 38.307\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 31, Discrimiantor Loss: 0.405, Generator Loss: 40.852\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 32, Discrimiantor Loss: 0.517, Generator Loss: 39.032\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 33, Discrimiantor Loss: 0.417, Generator Loss: 42.346\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 34, Discrimiantor Loss: 0.533, Generator Loss: 41.664\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 35, Discrimiantor Loss: 0.421, Generator Loss: 40.542\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 36, Discrimiantor Loss: 0.407, Generator Loss: 43.758\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 37, Discrimiantor Loss: 0.516, Generator Loss: 39.712\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 38, Discrimiantor Loss: 0.423, Generator Loss: 43.665\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 39, Discrimiantor Loss: 0.427, Generator Loss: 44.589\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 40, Discrimiantor Loss: 0.586, Generator Loss: 39.404\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 41, Discrimiantor Loss: 0.542, Generator Loss: 38.856\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 42, Discrimiantor Loss: 0.540, Generator Loss: 38.427\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 43, Discrimiantor Loss: 0.434, Generator Loss: 40.272\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 44, Discrimiantor Loss: 0.508, Generator Loss: 39.184\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 45, Discrimiantor Loss: 0.521, Generator Loss: 39.451\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 46, Discrimiantor Loss: 0.405, Generator Loss: 40.098\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 47, Discrimiantor Loss: 0.441, Generator Loss: 42.680\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 48, Discrimiantor Loss: 0.422, Generator Loss: 44.632\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 49, Discrimiantor Loss: 0.624, Generator Loss: 39.380\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 50, Discrimiantor Loss: 0.415, Generator Loss: 41.450\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 51, Discrimiantor Loss: 0.446, Generator Loss: 43.522\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 52, Discrimiantor Loss: 0.449, Generator Loss: 43.949\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 53, Discrimiantor Loss: 0.437, Generator Loss: 45.269\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 54, Discrimiantor Loss: 0.684, Generator Loss: 39.888\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 55, Discrimiantor Loss: 0.552, Generator Loss: 37.702\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 56, Discrimiantor Loss: 0.432, Generator Loss: 41.655\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 57, Discrimiantor Loss: 0.447, Generator Loss: 43.156\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 58, Discrimiantor Loss: 0.430, Generator Loss: 44.968\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 59, Discrimiantor Loss: 0.600, Generator Loss: 40.535\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 60, Discrimiantor Loss: 0.405, Generator Loss: 42.977\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 61, Discrimiantor Loss: 0.621, Generator Loss: 42.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 62, Discrimiantor Loss: 0.476, Generator Loss: 38.831\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 63, Discrimiantor Loss: 0.557, Generator Loss: 41.619\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 64, Discrimiantor Loss: 0.440, Generator Loss: 39.241\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 65, Discrimiantor Loss: 0.517, Generator Loss: 39.699\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 66, Discrimiantor Loss: 0.532, Generator Loss: 39.428\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 67, Discrimiantor Loss: 0.494, Generator Loss: 40.443\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 68, Discrimiantor Loss: 0.455, Generator Loss: 40.003\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 69, Discrimiantor Loss: 0.478, Generator Loss: 41.704\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 70, Discrimiantor Loss: 0.529, Generator Loss: 40.457\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 71, Discrimiantor Loss: 0.446, Generator Loss: 41.809\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 72, Discrimiantor Loss: 0.410, Generator Loss: 44.049\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 73, Discrimiantor Loss: 0.410, Generator Loss: 45.748\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 74, Discrimiantor Loss: 0.509, Generator Loss: 45.679\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 75, Discrimiantor Loss: 0.428, Generator Loss: 42.713\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 76, Discrimiantor Loss: 0.471, Generator Loss: 44.738\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 77, Discrimiantor Loss: 0.409, Generator Loss: 46.384\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 78, Discrimiantor Loss: 0.560, Generator Loss: 41.635\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 79, Discrimiantor Loss: 0.450, Generator Loss: 42.443\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 80, Discrimiantor Loss: 0.399, Generator Loss: 45.375\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 81, Discrimiantor Loss: 0.431, Generator Loss: 46.414\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 82, Discrimiantor Loss: 0.648, Generator Loss: 40.556\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 83, Discrimiantor Loss: 0.411, Generator Loss: 43.049\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 84, Discrimiantor Loss: 0.484, Generator Loss: 40.801\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 85, Discrimiantor Loss: 0.455, Generator Loss: 43.590\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 86, Discrimiantor Loss: 0.463, Generator Loss: 43.259\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 87, Discrimiantor Loss: 0.433, Generator Loss: 44.672\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 88, Discrimiantor Loss: 0.528, Generator Loss: 42.739\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 89, Discrimiantor Loss: 0.420, Generator Loss: 45.053\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 90, Discrimiantor Loss: 0.398, Generator Loss: 46.336\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 91, Discrimiantor Loss: 0.418, Generator Loss: 47.765\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 92, Discrimiantor Loss: 0.635, Generator Loss: 41.390\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 93, Discrimiantor Loss: 0.448, Generator Loss: 43.352\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 94, Discrimiantor Loss: 0.499, Generator Loss: 42.358\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 95, Discrimiantor Loss: 0.398, Generator Loss: 43.262\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 96, Discrimiantor Loss: 0.424, Generator Loss: 45.633\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 97, Discrimiantor Loss: 0.426, Generator Loss: 46.546\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 98, Discrimiantor Loss: 0.517, Generator Loss: 45.421\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 99, Discrimiantor Loss: 0.466, Generator Loss: 44.799\n",
      "11088/11088: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 100, Discrimiantor Loss: 0.433, Generator Loss: 43.672\n"
     ]
    }
   ],
   "source": [
    "alad.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def eval(model):\n",
    "    labels = []\n",
    "    scores = []\n",
    "\n",
    "    model.G.eval()\n",
    "    model.Dxx.eval()\n",
    "    model.E.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in data[1]:\n",
    "            img = img.float().to(device)\n",
    "            _, feature_real = model.Dxx(img, img)\n",
    "            _, feature_gen = model.Dxx(img, model.G(model.E(img)))\n",
    "            score = torch.sum(torch.abs(feature_real - feature_gen), dim=1)\n",
    "            scores.append(score.cpu())\n",
    "            labels.append(label.cpu())\n",
    "    scores = torch.cat(scores, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, scores = eval(alad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 67.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(scores[labels==0], color='r', alpha = 0.3)\n",
    "plt.hist(scores[labels==1], color='b', alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
